{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import types\n",
    "import torch\n",
    "import TextNet\n",
    "import TeacherNet\n",
    "import ImageNet\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import sys\n",
    "#import objgraph\n",
    "import gc\n",
    "import copy\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(teacher_model, img_model, txt_model, dataloaders,\n",
    "                criterion, optimizer, num_epochs=50000):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(teacher_model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "#         print('after a epoch')\n",
    "#         objgraph.show_growth()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                teacher_model.train()  # Set model to training mode\n",
    "                img_model.train()\n",
    "                txt_model.train()\n",
    "                print('Train phase')\n",
    "            else:\n",
    "                teacher_model.eval()  # Set model to evaluate mode\n",
    "                img_model.eval()\n",
    "                txt_model.eval()\n",
    "                print('Val phase')\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            num_batch = 0\n",
    "            for sample_batched in dataloaders[phase]:\n",
    "                num_batch += 1\n",
    "\n",
    "                img = sample_batched['image'].float().to(device)\n",
    "                embeds = sample_batched['embeds'].float().to(device)\n",
    "\n",
    "#                 print('initial at  a batch')\n",
    "#                 objgraph.show_growth()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    #  In train mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #  but in testing we only consider the final output.\n",
    "\n",
    "                    print('%s: [%s] forwarding image and embeddings...' % (str(epoch), str(num_batch)))\n",
    "                    img_reprets = teacher_model.forward(img_model.forward(img))\n",
    "                    txt_reprets = teacher_model.forward(txt_model.forward(embeds))\n",
    "\n",
    "                    loss = criterion(img_reprets, txt_reprets)\n",
    "\n",
    "                    preds = teacher_model.predict(img_reprets, txt_reprets)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        print('%s: [%s] backward and optimize...' % (str(epoch), str(num_batch)))\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * batch_size\n",
    "                running_corrects += sum([(i == preds[i]) + 0 for i in range(len(preds))])\n",
    "\n",
    "                print('after a batch')\n",
    "                objgraph.show_growth()\n",
    "\n",
    "                # release memory\n",
    "                del img, embeds, img_reprets, txt_reprets, preds\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if epoch % 100 == 0:\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(teacher_model.state_dict())\n",
    "                if phase == 'val':\n",
    "                    val_acc_history.append(epoch_acc)\n",
    "\n",
    "        gc.collect()\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    teacher_model.load_state_dict(best_model_wts)\n",
    "    return teacher_model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train data torch.Size([10000, 3, 224, 224]) torch.Size([10000, 52]) torch.Size([10000, 52])\n",
      "Loaded val data torch.Size([5000, 3, 224, 224]) torch.Size([5000, 43]) torch.Size([5000, 43])\n"
     ]
    }
   ],
   "source": [
    "train_img = torch.load(\"../hci-intermodal-reasoning/cached_data/train_img\")\n",
    "train_cap = torch.load(\"../hci-intermodal-reasoning/cached_data/train_cap\")\n",
    "train_mask = torch.load(\"../hci-intermodal-reasoning/cached_data/train_mask\")\n",
    "\n",
    "val_img = torch.load(\"../hci-intermodal-reasoning/cached_data/val_img\")\n",
    "val_cap = torch.load(\"../hci-intermodal-reasoning/cached_data/val_cap\")\n",
    "val_mask = torch.load(\"../hci-intermodal-reasoning/cached_data/val_mask\")\n",
    "\n",
    "print(\"Loaded train data\", train_img.size(), train_cap.size(), train_mask.size())\n",
    "print(\"Loaded val data\", val_img.size(), val_cap.size(), val_mask.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA = 0.002\n",
    "BATCH_SIZE = 8\n",
    "NB_EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RankingLossFunc()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = TensorDataset(train_img, train_cap, train_mask)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE, num_workers=2)\n",
    "valid_data = TensorDataset(val_img, val_cap, val_mask)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE * 2, num_workers=2)\n",
    "\n",
    "device = \"cuda:1\"\n",
    "text_net = TextNet.Text_Net(device)\n",
    "vision_net = ImageNet.Image_Net(device)\n",
    "teacher_net = TeacherNet.Teacher_Net()\n",
    "ranking_loss = TeacherNet.RankingLossFunc(DELTA)\n",
    "teacher_net.to(device)\n",
    "ranking_loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alexnet\"\n",
    "feature_extract = True\n",
    "vision_net = vision_net.initialize_model(model_name, feature_extract, use_pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n"
     ]
    }
   ],
   "source": [
    "print(\"Params to learn:\")\n",
    "params_to_update_share = []\n",
    "params_to_update_img = vision_net.parameters()\n",
    "params_to_update_txt = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "Teacher_Net(\n",
      "  (linear1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (linear2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (linear3): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=9216, bias=True)\n",
      "  )\n",
      ")\n",
      "Text_Net(\n",
      "  (conv1): Conv1d(16, 300, kernel_size=(3,), stride=(1,))\n",
      "  (conv2): Conv1d(300, 300, kernel_size=(3,), stride=(1,))\n",
      "  (conv3): Conv1d(300, 300, kernel_size=(3,), stride=(1,))\n",
      "  (fc1): Linear(in_features=21900, out_features=9216, bias=True)\n",
      ")\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "\t linear3.weight\n",
      "\t linear3.bias\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t fc1.weight\n",
      "\t fc1.bias\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Params to learn:\")\n",
    "    params_to_update_share = []\n",
    "    params_to_update_img = vision_net.parameters()\n",
    "    params_to_update_txt = []\n",
    "    \n",
    "    print(teacher_net)\n",
    "    print(vision_net)\n",
    "    print(text_net)\n",
    "    \n",
    "    for name, param in teacher_net.named_parameters():\n",
    "        if param.requires_grad is True:\n",
    "            params_to_update_share.append(param)\n",
    "            print(\"\\t\", name)\n",
    "\n",
    "    if feature_extract:\n",
    "        params_to_update_img = []\n",
    "        for name, param in vision_net.named_parameters():\n",
    "            if param.requires_grad is True:\n",
    "                params_to_update_img.append(param)\n",
    "                print(\"\\t\", name)\n",
    "    else:\n",
    "        for name, param in vision_net.named_parameters():\n",
    "            if param.requires_grad is True:\n",
    "                print(\"\\t\", name)\n",
    "\n",
    "    for name, param in text_net.named_parameters():\n",
    "        if param.requires_grad is True:\n",
    "            params_to_update_txt.append(param)\n",
    "            print(\"\\t\", name)\n",
    "            \n",
    "    \n",
    "    params_to_update = list(params_to_update_share) + list(params_to_update_img) + list(params_to_update_txt)\n",
    "    optimizer = optim.Adam(params_to_update, lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
