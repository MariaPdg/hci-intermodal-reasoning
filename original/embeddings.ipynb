{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  1.3.0+cu92\n",
      "Torchtext Version:  0.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import GloVe\n",
    "from transformers import DistilBertTokenizer\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import numpy as np\n",
    "import TextNet\n",
    "\n",
    "print(\"Torch Version: \", torch.__version__)\n",
    "print(\"Torchtext Version: \", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_CACHE = os.path.expanduser(\"../../words/glove/\")\n",
    "DATASET_CACHE = os.path.expanduser(\"./\")\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train data torch.Size([10000, 3, 224, 224]) torch.Size([10000, 52]) torch.Size([10000, 52])\n",
      "Loaded val data torch.Size([5000, 3, 224, 224]) torch.Size([5000, 43]) torch.Size([5000, 43])\n"
     ]
    }
   ],
   "source": [
    "train_img = torch.load(\"../cached_data/train_img\")\n",
    "train_cap = torch.load(\"../cached_data/train_cap\")\n",
    "train_mask = torch.load(\"../cached_data/train_mask\")\n",
    "\n",
    "val_img = torch.load(\"../cached_data/val_img\")\n",
    "val_cap = torch.load(\"../cached_data/val_cap\")\n",
    "val_mask = torch.load(\"../cached_data/val_mask\")\n",
    "\n",
    "print(\"Loaded train data\", train_img.size(), train_cap.size(), train_mask.size())\n",
    "print(\"Loaded val data\", val_img.size(), val_cap.size(), val_mask.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5837\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(train_cap):\n",
    "    \n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    vocab = []\n",
    "    for i in range (len(train_cap)):\n",
    "        words = tokenizer.decode(train_cap[i].numpy()).split(' ')\n",
    "        for j in range (len(words)):\n",
    "            if  words[j]== '[CLS]' or  words[j]== '[PAD]' or words[j]=='[SEP]':\n",
    "                continue;\n",
    "            else:  \n",
    "                if words[j] not in vocab: #add unique words\n",
    "                    vocab.append(words[j])\n",
    "\n",
    "    vocab.append('[CLS]')\n",
    "    vocab.append('[PAD]')\n",
    "    vocab.append('[SEP]')\n",
    "\n",
    "    return vocab\n",
    "\n",
    "#print(vocab)\n",
    "#print(train_cap[0][0])\n",
    "vocab = build_vocab(train_cap)\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "(5838, 300)\n"
     ]
    }
   ],
   "source": [
    "def build_embed(vocab):\n",
    "    \n",
    "    glove = torchtext.vocab.Vectors('../../words/glove/glove.6B.300d.txt')\n",
    "    print(len(glove[vocab[10]]))\n",
    "\n",
    "    EMBEDDING_DIM = 300\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, EMBEDDING_DIM))\n",
    "    i = 0\n",
    "    for word in vocab:\n",
    "        embedding_vector = glove[word]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            i = i+1\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = build_embed(vocab)\n",
    "print(np.shape(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "def build_train(train_cap):\n",
    "    \n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    vocab = []\n",
    "    for i in range (len(train_cap)):\n",
    "        words = tokenizer.decode(train_cap[i].numpy()).split(' ')\n",
    "        vocab.append(words)\n",
    "    return vocab\n",
    "\n",
    "#print(vocab)\n",
    "#print(train_cap[0][0])\n",
    "train = build_train(train_cap)\n",
    "print(len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.field.Field object at 0x7f6b74551b38>\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(sequential=True, lower=True, include_lengths=True, batch_first=True, tokenize = 'spacy')\n",
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5839, 300])\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300, cache=EMB_CACHE))\n",
    "vocab_new = TEXT.vocab\n",
    "print(vocab_new.vectors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = TEXT.build_vocab(vocab, vectors=GloVe(name='6B', dim=300, cache=EMB_CACHE))\n",
    "# vocab = TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocab.vectors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize = lambda x: x.split()\n",
    "#TEXT = torchtext.data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT = torchtext.data.Field(sequential=True, lower=True, include_lengths=True, batch_first=True, tokenize = 'spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = TEXT.build_vocab(vocab, vectors=GloVe(name='6B', dim=300, cache=EMB_CACHE))\n",
    "# print(vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# embedding = nn.Embedding(1000,128)\n",
    "# embedding(torch.LongTensor([3,4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('../dataset/annotations/captions_train2014.json') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "#     print('Caption: ' + p['caption'])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../../words/vocab.pkl', 'rb') as f:\n",
    "#         vocab = pickle.load(f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_glove(word_index):\n",
    "#     EMBEDDING_FILE = '../../words/glove/glove.6B.300d.txt'\n",
    "#     def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "#     embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    \n",
    "#     all_embs = np.stack(embeddings_index.values())\n",
    "#     emb_mean,emb_std = -0.005838499,0.48782197\n",
    "#     embed_size = all_embs.shape[1]\n",
    "\n",
    "#     # word_index = tokenizer.word_index\n",
    "#     nb_words = min(max_features, len(word_index))\n",
    "#     embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "#     for word, i in word_index.items():\n",
    "#         if i >= max_features: continue\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         #ALLmight\n",
    "#         if embedding_vector is not None: \n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "#         else:\n",
    "#             embedding_vector = embeddings_index.get(word.capitalize())\n",
    "#             if embedding_vector is not None: \n",
    "#                 embedding_matrix[i] = embedding_vector\n",
    "#     return embedding_matrix \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_fasttext(word_index):    \n",
    "#     EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "#     def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "#     embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "#     all_embs = np.stack(embeddings_index.values())\n",
    "#     emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "#     embed_size = all_embs.shape[1]\n",
    "\n",
    "#     # word_index = tokenizer.word_index\n",
    "#     nb_words = min(max_features, len(word_index))\n",
    "#     embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "#     for word, i in word_index.items():\n",
    "#         if i >= max_features: continue\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#     return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Captions(torchtext.data.TabularDataset):\n",
    "\n",
    "#     @classmethod\n",
    "#     def splits(annotations.captions, root='../dataset/annotations', \n",
    "#                train='captions_train2014.json', **kwargs):\n",
    "    \n",
    "#         fields = {'annotations': annotations.captions}\n",
    "        \n",
    "        \n",
    "#         return super(Captions, cls).splits(\n",
    "#             fields=fields, root=root, train=train,\n",
    "#             format='json',**kwargs)\n",
    "\n",
    "# TEXT = torchtext.data.Field(sequential=True, lower=True, include_lengths=True, batch_first=True, tokenize='spacy')\n",
    "# train = Captions.splits(root='../dataset/annotations', info =None, images = None, licenses = None, annotations=('annotations',TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields = {'annotations':TEXT}\n",
    "# train_data = torchtext.data.TabularDataset.splits(\n",
    "#                             path = '../dataset/annotations',\n",
    "#                             train = 'captions_train2014.json',\n",
    "#                             format = 'json',\n",
    "#                             fields = fields\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (py2env)",
   "language": "python",
   "name": "hci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
